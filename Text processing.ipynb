{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(conf=SparkConf())\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame({\n",
    "        'texts': [\"I like playing?\",\n",
    "                  \"I like coding.\"]\n",
    "    })\n",
    "    \n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+------------------+---------------+------------+---------------+\n",
      "|          texts|cleaned|            tokens| refined_tokens|        stem|            lem|\n",
      "+---------------+-------+------------------+---------------+------------+---------------+\n",
      "|I like playing?|   null|[i, like, playing]|[like, playing]|[like, play]|[like, playing]|\n",
      "| I like coding.|   null| [i, like, coding]| [like, coding]|[like, code]| [like, coding]|\n",
      "+---------------+-------+------------------+---------------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unidecode\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def clean(text):\n",
    "    text= unidecode.unidecode(text) # remove accents\n",
    "    text = text.lower()\n",
    "    text = text.replace(r'\\n','') # remove newline sign\n",
    "    text = re.sub(r'\\d+', '', text) # remove digits \n",
    "    text = re.sub(r'[.]?-[.]?', '', text) # concatenate divided words\n",
    "    text = re.sub(r'[\\W]+',' ', text) # replace non-alphanum with space  \n",
    "    text = re.sub(' +', ' ', text) # replace multiple spaces with single space \n",
    "    return text\n",
    "\n",
    "user_def_fun = udf(f=clean, returnType=ArrayType(StringType()))\n",
    "\n",
    "df = df.withColumn(\"cleaned\", user_def_fun(\"texts\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# A tokenizer that converts the input string to lowercase and then\n",
    "# splits it by white spaces.\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned\", outputCol=\"tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopword_removal = StopWordsRemover(inputCol='tokens', \n",
    "                                    outputCol='refined_tokens')\n",
    "df = stopword_removal.transform(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming vs lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "| refined_tokens|        stem|\n",
      "+---------------+------------+\n",
      "|[like, playing]|[like, play]|\n",
      "| [like, coding]|[like, code]|\n",
      "+---------------+------------+\n",
      "\n",
      "CPU times: user 8.91 ms, sys: 5.54 ms, total: 14.4 ms\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def list_stemmer(words):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "stemming = udf(list_stemmer, returnType=ArrayType(StringType()))\n",
    "\n",
    "df = df.withColumn(\"stem\", stemming(\"refined_tokens\"))\n",
    "df.select(['refined_tokens','stem']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "| refined_tokens|            lem|\n",
      "+---------------+---------------+\n",
      "|[like, playing]|[like, playing]|\n",
      "| [like, coding]| [like, coding]|\n",
      "+---------------+---------------+\n",
      "\n",
      "CPU times: user 14.5 ms, sys: 844 Âµs, total: 15.3 ms\n",
      "Wall time: 4.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def list_lemmatizer(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "lemmatization = udf(list_lemmatizer, returnType=ArrayType(StringType()))\n",
    "\n",
    "df = df.withColumn(\"lem\", lemmatization(\"refined_tokens\"))\n",
    "df.select(['refined_tokens','lem']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------------------+\n",
      "|stem        |tf_features                       |\n",
      "+------------+----------------------------------+\n",
      "|[like, play]|(262144,[123981,208258],[1.0,1.0])|\n",
      "|[like, code]|(262144,[93284,208258],[1.0,1.0]) |\n",
      "+------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashing_vec = HashingTF(numFeatures=262144,\n",
    "                        inputCol='stem',\n",
    "                        outputCol='tf_features')\n",
    "\n",
    "hashing_df = hashing_vec.transform(df)\n",
    "hashing_df.select(['stem','tf_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------------------------------------+\n",
      "|stem        |tf_idf_features                                  |\n",
      "+------------+-------------------------------------------------+\n",
      "|[like, play]|(262144,[123981,208258],[0.4054651081081644,0.0])|\n",
      "|[like, code]|(262144,[93284,208258],[0.4054651081081644,0.0]) |\n",
      "+------------+-------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vec = IDF(inputCol='tf_features',\n",
    "               outputCol='tf_idf_features')\n",
    "\n",
    "tf_idf_df = tf_idf_vec.fit(hashing_df).transform(hashing_df)\n",
    "tf_idf_df.select(['stem','tf_idf_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coding', 'like', 'playing']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"like playing\",\n",
    "          \"like coding\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.57973867, 0.81480247],\n",
       "       [0.81480247, 0.57973867, 0.        ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.8148024746671689\n",
      "  (0, 1)\t0.5797386715376657\n",
      "  (1, 0)\t0.8148024746671689\n",
      "  (1, 1)\t0.5797386715376657\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondad8740a4f8eca4c079729b3e8f23ddf31"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
