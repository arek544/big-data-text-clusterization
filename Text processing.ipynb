{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(conf=SparkConf())\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame({\n",
    "        'texts': [\"I like playing?\",\n",
    "                  \"I like coding.\"]\n",
    "    })\n",
    "    \n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unidecode\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def clean(text):\n",
    "    text= unidecode.unidecode(text) # remove accents\n",
    "    text = text.lower()\n",
    "    text = text.replace(r'\\n','') # remove newline sign\n",
    "    text = re.sub(r'\\d+', '', text) # remove digits \n",
    "    text = re.sub(r'[.]?-[.]?', '', text) # concatenate divided words\n",
    "    text = re.sub(r'[\\W]+',' ', text) # replace non-alphanum with space  \n",
    "    text = re.sub(' +', ' ', text) # replace multiple spaces with single space \n",
    "    return text\n",
    "\n",
    "user_def_fun = udf(clean)\n",
    "\n",
    "df = df.withColumn(\"cleaned\", user_def_fun(\"texts\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# A tokenizer that converts the input string to lowercase and then\n",
    "# splits it by white spaces.\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned\", outputCol=\"tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopword_removal = StopWordsRemover(inputCol='tokens', \n",
    "                                    outputCol='refined_tokens')\n",
    "df = stopword_removal.transform(df)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming vs lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "| refined_tokens|        stem|\n",
      "+---------------+------------+\n",
      "|[like, playing]|[like, play]|\n",
      "| [like, coding]|[like, code]|\n",
      "+---------------+------------+\n",
      "\n",
      "CPU times: user 15 ms, sys: 2.3 ms, total: 17.3 ms\n",
      "Wall time: 1.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def list_stemmer(words):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "stemming = udf(list_stemmer)\n",
    "\n",
    "df = df.withColumn(\"stem\", stemming(\"refined_tokens\"))\n",
    "df.select(['refined_tokens','stem']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "| refined_tokens|            lem|\n",
      "+---------------+---------------+\n",
      "|[like, playing]|[like, playing]|\n",
      "| [like, coding]| [like, coding]|\n",
      "+---------------+---------------+\n",
      "\n",
      "CPU times: user 10 ms, sys: 4.54 ms, total: 14.6 ms\n",
      "Wall time: 3.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def list_lemmatizer(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "lemmatization = udf(list_lemmatizer)\n",
    "\n",
    "df = df.withColumn(\"lem\", lemmatization(\"refined_tokens\"))\n",
    "df.select(['refined_tokens','lem']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashing_vec = HashingTF(numFeatures=4,\n",
    "                        inputCol='refined_tokens',\n",
    "                        outputCol='tf_features')\n",
    "\n",
    "hashing_df = hashing_vec.transform(df)\n",
    "# hashing_df.select(['refined_tokens','tf_features']).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------------------+\n",
      "|refined_tokens |tf_idf_features                   |\n",
      "+---------------+----------------------------------+\n",
      "|[like, playing]|(4,[1,2],[0.4054651081081644,0.0])|\n",
      "|[like, coding] |(4,[2,3],[0.0,0.4054651081081644])|\n",
      "+---------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vec = IDF(inputCol='tf_features',\n",
    "               outputCol='tf_idf_features')\n",
    "\n",
    "tf_idf_df = tf_idf_vec.fit(hashing_df).transform(hashing_df)\n",
    "tf_idf_df.select(['refined_tokens','tf_idf_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coding', 'like', 'playing']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"like playing\",\n",
    "          \"like coding\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.57973867, 0.81480247],\n",
       "       [0.81480247, 0.57973867, 0.        ]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.8148024746671689\n",
      "  (0, 1)\t0.5797386715376657\n",
      "  (1, 0)\t0.8148024746671689\n",
      "  (1, 1)\t0.5797386715376657\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondad8740a4f8eca4c079729b3e8f23ddf31"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
