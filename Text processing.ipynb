{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext(conf=SparkConf())\n",
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               texts|\n",
      "+--------------------+\n",
      "|I like playing?an...|\n",
      "|      I like coding.|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame({\n",
    "        'texts': [\"I like playing?and drinking\",\n",
    "                  \"I like coding.\"]})\n",
    "    \n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               texts|             cleaned|\n",
      "+--------------------+--------------------+\n",
      "|I like playing?an...|i like playing an...|\n",
      "|      I like coding.|      i like coding |\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unidecode\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def clean(text):\n",
    "    text= unidecode.unidecode(text) # remove accents\n",
    "    text = text.lower()\n",
    "    text = text.replace(r'\\n','') # remove newline sign\n",
    "    text = re.sub(r'\\d+', '', text) # remove digits \n",
    "    text = re.sub(r'[.]?-[.]?', '', text) # concatenate divided words\n",
    "    text = re.sub(r'[\\W]+',' ', text) # replace non-alphanum with space  \n",
    "    text = re.sub(' +', ' ', text) # replace multiple spaces with single space \n",
    "    return text\n",
    "\n",
    "user_def_fun = udf(f=clean, returnType=StringType())\n",
    "\n",
    "df = df.withColumn(\"cleaned\", user_def_fun(\"texts\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------------------------+---------------------------------+\n",
      "|texts                      |cleaned                    |tokens                           |\n",
      "+---------------------------+---------------------------+---------------------------------+\n",
      "|I like playing?and drinking|i like playing and drinking|[i, like, playing, and, drinking]|\n",
      "|I like coding.             |i like coding              |[i, like, coding]                |\n",
      "+---------------------------+---------------------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# A tokenizer that converts the input string to lowercase and then\n",
    "# splits it by white spaces.\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned\", outputCol=\"tokens\")\n",
    "df = tokenizer.transform(df)\n",
    "df.show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------------------------+---------------------------------+-------------------------+\n",
      "|texts                      |cleaned                    |tokens                           |refined_tokens           |\n",
      "+---------------------------+---------------------------+---------------------------------+-------------------------+\n",
      "|I like playing?and drinking|i like playing and drinking|[i, like, playing, and, drinking]|[like, playing, drinking]|\n",
      "|I like coding.             |i like coding              |[i, like, coding]                |[like, coding]           |\n",
      "+---------------------------+---------------------------+---------------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "stopword_removal = StopWordsRemover(inputCol='tokens', \n",
    "                                    outputCol='refined_tokens')\n",
    "df = stopword_removal.transform(df)\n",
    "\n",
    "df.show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming vs lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|      refined_tokens|               stem|\n",
      "+--------------------+-------------------+\n",
      "|[like, playing, d...|[like, play, drink]|\n",
      "|      [like, coding]|       [like, code]|\n",
      "+--------------------+-------------------+\n",
      "\n",
      "CPU times: user 9.56 ms, sys: 3.8 ms, total: 13.4 ms\n",
      "Wall time: 3.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def list_stemmer(words):\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "stemming = udf(list_stemmer, returnType=ArrayType(StringType()))\n",
    "\n",
    "df = df.withColumn(\"stem\", stemming(\"refined_tokens\"))\n",
    "df.select(['refined_tokens','stem']).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|      refined_tokens|                 lem|\n",
      "+--------------------+--------------------+\n",
      "|[like, playing, d...|[like, playing, d...|\n",
      "|      [like, coding]|      [like, coding]|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "CPU times: user 14.7 ms, sys: 0 ns, total: 14.7 ms\n",
      "Wall time: 5.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def list_lemmatizer(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "lemmatization = udf(list_lemmatizer, returnType=ArrayType(StringType()))\n",
    "\n",
    "df = df.withColumn(\"lem\", lemmatization(\"refined_tokens\"))\n",
    "df.select(['refined_tokens','lem']).show(2,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------------------------+\n",
      "|stem               |tf_features                                 |\n",
      "+-------------------+--------------------------------------------+\n",
      "|[like, play, drink]|(262144,[33140,123981,208258],[1.0,1.0,1.0])|\n",
      "|[like, code]       |(262144,[93284,208258],[1.0,1.0])           |\n",
      "+-------------------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashing_vec = HashingTF(numFeatures=262144,\n",
    "                        inputCol='stem',\n",
    "                        outputCol='tf_features')\n",
    "\n",
    "hashing_df = hashing_vec.transform(df)\n",
    "hashing_df.select(['stem','tf_features']).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------------------------------------------------------+\n",
      "|stem               |tf_idf_features                                                           |\n",
      "+-------------------+--------------------------------------------------------------------------+\n",
      "|[like, play, drink]|(262144,[33140,123981,208258],[0.4054651081081644,0.4054651081081644,0.0])|\n",
      "|[like, code]       |(262144,[93284,208258],[0.4054651081081644,0.0])                          |\n",
      "+-------------------+--------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vec = IDF(inputCol='tf_features',\n",
    "               outputCol='tf_idf_features')\n",
    "\n",
    "tf_idf_df = tf_idf_vec.fit(hashing_df).transform(hashing_df)\n",
    "tf_idf_df.select(['stem','tf_idf_features']).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondad8740a4f8eca4c079729b3e8f23ddf31"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
